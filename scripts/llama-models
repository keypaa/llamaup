#!/usr/bin/env bash
# llama-models -- HuggingFace GGUF model browser and downloader
#
# Optional TUI tool for browsing and downloading GGUF models from HuggingFace.
# Supports two modes:
#   - Premium: gum TUI + aria2c for fast downloads (opt-in)
#   - Minimal: bash-native select menu + curl (zero dependencies)
#
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly SCRIPT_DIR

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------
readonly MODELS_DIR="${LLAMA_MODELS_DIR:-${HOME}/.local/share/llama-models}"
readonly HF_API_URL="https://huggingface.co/api/models"
readonly VERSION="0.1.0"

# Mode will be set by detect_mode()
MODE=""

# ---------------------------------------------------------------------------
# Colour constants (only if stdout is a TTY)
# ---------------------------------------------------------------------------
if [[ -t 1 ]]; then
  RED='\033[0;31m'
  GREEN='\033[0;32m'
  YELLOW='\033[1;33m'
  CYAN='\033[0;36m'
  BLUE='\033[0;34m'
  MAGENTA='\033[0;35m'
  BOLD='\033[1m'
  DIM='\033[2m'
  RESET='\033[0m'
else
  RED=''
  GREEN=''
  YELLOW=''
  CYAN=''
  BLUE=''
  MAGENTA=''
  BOLD=''
  DIM=''
  RESET=''
fi

# ---------------------------------------------------------------------------
# Helper functions
# ---------------------------------------------------------------------------
error()   { echo -e "${RED}Error: $1${RESET}" >&2; exit 1; }
info()    { echo -e "${CYAN}-> $1${RESET}" >&2; }
success() { echo -e "${GREEN}[OK] $1${RESET}" >&2; }
warn()    { echo -e "${YELLOW}[!] $1${RESET}" >&2; }

# ---------------------------------------------------------------------------
# usage -- print CLI help and exit 0
# ---------------------------------------------------------------------------
usage() {
  echo -e "${BOLD}llama-models${RESET} v${VERSION} -- HuggingFace GGUF Model Browser"
  echo
  echo -e "${BOLD}USAGE${RESET}"
  echo "  llama-models [OPTIONS] [QUERY]"
  echo
  echo -e "${BOLD}DESCRIPTION${RESET}"
  echo "  Browse and download GGUF models from HuggingFace with a beautiful TUI."
  echo
  echo "  Two modes available:"
  echo -e "    ${GREEN}Premium${RESET}: Modern TUI with gum + ultra-fast aria2c downloads (opt-in)"
  echo -e "    ${BLUE}Minimal${RESET}: Bash-native menu + standard curl (zero dependencies)"
  echo
  echo -e "${BOLD}OPTIONS${RESET}"
  echo "  search <query>     Search for models matching query"
  echo "  list               List locally downloaded models"
  echo "  sync               Check for updates to downloaded models"
  echo "  --mode <mode>      Force mode: 'premium' or 'minimal'"
  echo "  --force            Force re-download even if file exists"
  echo "  --install-deps     Install premium mode dependencies (gum + aria2c)"
  echo "  --version          Show version"
  echo "  -h, --help         Show this help"
  echo
  echo -e "${BOLD}EXAMPLES${RESET}"
  echo "  llama-models                    # Launch interactive browser"
  echo "  llama-models search qwen        # Search for \"qwen\" models"
  echo "  llama-models list               # Show downloaded models"
  echo "  llama-models sync               # Check for updates"
  echo "  llama-models --force search qwen # Re-download even if exists"
  echo "  llama-models --install-deps     # Install gum + aria2c"
  echo
  echo -e "${BOLD}STORAGE${RESET}"
  echo "  Models are downloaded to: ${MODELS_DIR}"
  echo
  echo -e "${BOLD}MORE INFO${RESET}"
  echo "  https://github.com/keypaa/llamaup"
  exit 0
}

# ---------------------------------------------------------------------------
# check_dependency -- check if a command exists
# Args: $1 = command name
# Returns: 0 if exists, 1 if not
# ---------------------------------------------------------------------------
check_dependency() {
  command -v "$1" >/dev/null 2>&1
}

# ---------------------------------------------------------------------------
# detect_mode -- auto-detect which mode to use
# Sets the global MODE variable to "premium" or "minimal"
# Args: $1 = action (optional: "list" or "sync" won't trigger installation prompt)
# ---------------------------------------------------------------------------
detect_mode() {
  local action="${1:-}"
  local has_gum=false
  local has_aria2c=false
  
  check_dependency gum && has_gum=true
  check_dependency aria2c && has_aria2c=true
  
  # Premium mode requires both gum and aria2c
  if [[ "$has_gum" == "true" ]] && [[ "$has_aria2c" == "true" ]]; then
    MODE="premium"
    return 0
  fi
  
  # For list/sync commands, don't offer installation (they don't need premium deps)
  if [[ "$action" == "list" ]] || [[ "$action" == "sync" ]]; then
    MODE="minimal"
    return 0
  fi
  
  # Check if user wants to install dependencies (for interactive/search)
  if [[ "$has_gum" == "false" ]] || [[ "$has_aria2c" == "false" ]]; then
    offer_installation
  else
    MODE="minimal"
  fi
}

# ---------------------------------------------------------------------------
# offer_installation -- prompt user to install premium dependencies
# ---------------------------------------------------------------------------
offer_installation() {
  echo
  echo -e "${BOLD}llama-models${RESET} -- Enhanced Mode Available"
  echo
  echo -e "For the best experience, install optional dependencies:"
  echo -e "  ${GREEN}*${RESET} ${BOLD}gum${RESET}    -- Modern TUI framework"
  echo -e "  ${GREEN}*${RESET} ${BOLD}aria2c${RESET} -- Ultra-fast downloads (16x faster than curl)"
  echo
  echo -e "${DIM}Without these, you'll use basic mode (bash menu + curl)${RESET}"
  echo
  
  # Simple bash-native prompt
  read -p "Install enhanced mode dependencies? [Y/n] " -n 1 -r
  echo
  
  if [[ $REPLY =~ ^[Yy]$ ]] || [[ -z $REPLY ]]; then
    install_dependencies
  else
    info "Using minimal mode (bash + curl)"
    MODE="minimal"
  fi
}

# ---------------------------------------------------------------------------
# install_dependencies -- install gum and aria2c
# ---------------------------------------------------------------------------
install_dependencies() {
  info "Installing dependencies..."
  
  local os_type
  os_type="$(uname -s)"
  
  case "$os_type" in
    Linux)
      install_linux_dependencies
      ;;
    Darwin)
      install_macos_dependencies
      ;;
    *)
      warn "Unsupported OS: $os_type"
      warn "Please install manually:"
      echo "  - gum: https://github.com/charmbracelet/gum"
      echo "  - aria2c: https://aria2.github.io/"
      MODE="minimal"
      return 1
      ;;
  esac
  
  # Verify installation
  if check_dependency gum && check_dependency aria2c; then
    success "Dependencies installed successfully"
    MODE="premium"
  else
    warn "Installation incomplete. Using minimal mode."
    MODE="minimal"
  fi
}

# ---------------------------------------------------------------------------
# install_linux_dependencies -- install on Linux
# ---------------------------------------------------------------------------
install_linux_dependencies() {
  # Detect package manager
  if check_dependency apt; then
    info "Installing via apt..."
    sudo apt update && sudo apt install -y aria2
    install_gum_linux
  elif check_dependency yum; then
    info "Installing via yum..."
    sudo yum install -y aria2
    install_gum_linux
  elif check_dependency pacman; then
    info "Installing via pacman..."
    sudo pacman -S --noconfirm aria2
    install_gum_linux
  else
    warn "No supported package manager found"
    return 1
  fi
}

# ---------------------------------------------------------------------------
# install_gum_linux -- install gum binary on Linux via Charm APT repo
# ---------------------------------------------------------------------------
install_gum_linux() {
  # Check if we have sudo access
  if ! sudo -n true 2>/dev/null && ! [[ $EUID -eq 0 ]]; then
    warn "sudo access required to install gum via APT. Please run with sudo privileges or install gum manually."
    echo "Manual installation: https://github.com/charmbracelet/gum#installation"
    return 1
  fi

  info "Adding Charm APT repository..."
  sudo mkdir -p /etc/apt/keyrings
  curl -fsSL https://repo.charm.sh/apt/gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/charm.gpg
  echo "deb [signed-by=/etc/apt/keyrings/charm.gpg] https://repo.charm.sh/apt/ * *" | sudo tee /etc/apt/sources.list.d/charm.list > /dev/null

  info "Updating package list..."
  sudo apt update

  info "Installing gum..."
  sudo apt install -y gum

  success "gum installed successfully!"
}

# ---------------------------------------------------------------------------
# install_macos_dependencies -- install on macOS via Homebrew
# ---------------------------------------------------------------------------
install_macos_dependencies() {
  if ! check_dependency brew; then
    warn "Homebrew not found. Install from: https://brew.sh"
    return 1
  fi
  
  info "Installing via Homebrew..."
  brew install gum aria2
}

# ===========================================================================
# PHASE 2: HuggingFace API Integration
# ===========================================================================

# ---------------------------------------------------------------------------
# search_models -- query HuggingFace API for GGUF models
# Args: $1 = search query (optional, searches all if empty)
#       $2 = limit (optional, default 50)
# Returns: JSON array of models (via stdout)
# Exit: 1 on API failure or invalid JSON
# ---------------------------------------------------------------------------
search_models() {
  local query="${1:-}"
  local limit="${2:-50}"
  local skip="${3:-0}"
  local api_url="${HF_API_URL}"
  
  # Build query parameters
  # CRITICAL: full=true returns siblings array with file metadata
  local params="limit=${limit}&sort=downloads&direction=-1&full=true"
  
  # Add skip/offset for pagination
  if [[ "$skip" -gt 0 ]]; then
    params="${params}&skip=${skip}"
  fi
  
  # Add search filter if provided
  if [[ -n "$query" ]]; then
    # URL-encode the query (basic: replace spaces with %20)
    local encoded_query
    encoded_query=$(echo "$query" | sed 's/ /%20/g')
    params="${params}&search=${encoded_query}"
  fi
  
  # Filter by gguf tag
  params="${params}&filter=gguf"
  
  info "Searching HuggingFace for GGUF models..."
  [[ -n "$query" ]] && info "Query: ${BOLD}${query}${RESET}"
  
  # Make API request with timeout
  local response
  if ! response=$(curl -fsSL --max-time 30 "${api_url}?${params}"); then
    error "Failed to query HuggingFace API"
  fi
  
  # Validate JSON response
  if ! echo "$response" | jq empty 2>/dev/null; then
    error "Invalid JSON response from HuggingFace API"
  fi
  
  echo "$response"
}

# ---------------------------------------------------------------------------
# fetch_model_details -- get full metadata for a single model
# Args: $1 = model_id (e.g. "bartowski/Llama-3.1-8B-GGUF")
# Returns: JSON object or empty string on failure
# ---------------------------------------------------------------------------
fetch_model_details() {
  local model_id="$1"
  local api_url="https://huggingface.co/api/models/${model_id}?full=true"

  local response
  if ! response=$(curl -fsSL --max-time 30 "$api_url" 2>/dev/null); then
    warn "Failed to fetch model details for size metadata: ${model_id}"
    echo ""
    return 1
  fi

  if ! echo "$response" | jq empty 2>/dev/null; then
    warn "Invalid model detail JSON for: ${model_id}"
    echo ""
    return 1
  fi

  echo "$response"
  return 0
}

# ---------------------------------------------------------------------------
# filter_gguf_models -- extract only GGUF models from API results
# Args: $1 = JSON array from search_models
# Returns: Filtered JSON array with only GGUF models (via stdout)
# Note: Filters models that have .gguf files in siblings array
# ---------------------------------------------------------------------------
filter_gguf_models() {
  local json="$1"
  
  # Filter models that have .gguf files
  # Look for either:
  #   - 'gguf' in tags array
  #   - siblings array contains files ending with .gguf
  echo "$json" | jq '[.[] | select(
    (.tags // [] | any(. == "gguf")) or
    (.siblings // [] | any(.rfilename | endswith(".gguf")))
  )]'
}

# ---------------------------------------------------------------------------
# parse_model_metadata -- extract metadata from a single model
# Args: $1 = model JSON object
# Returns: pipe-separated string: id|downloads|size|quant_count
# Example: "TheBloke/Llama-2-7B-GGUF|125000|7.2GiB|8"
# Note: Calculates size and count from siblings array (available with full=true)
# ---------------------------------------------------------------------------
parse_model_metadata() {
  local model_json="$1"
  
  local id downloads total_size quant_count
  
  # Extract basic info
  id=$(echo "$model_json" | jq -r '.id // .modelId // "unknown"')
  downloads=$(echo "$model_json" | jq -r '.downloads // 0')
  
  # Calculate total GGUF size and count from siblings
  quant_count=$(echo "$model_json" | jq -r '
    [.siblings // [] | .[] | select(.rfilename | endswith(".gguf"))] | length
  ')
  
  total_size=$(echo "$model_json" | jq -r '
    [.siblings // [] | .[] | select(.rfilename | endswith(".gguf")) | .size // 0] | add // 0
  ')
  
  # Format size (bytes to human-readable)
  local size_human
  if [[ "$total_size" -eq 0 ]]; then
    size_human="N/A"
  elif command -v numfmt >/dev/null 2>&1; then
    size_human=$(echo "$total_size" | numfmt --to=iec-i --suffix=B 2>/dev/null || echo "${total_size}B")
  else
    # Fallback: simple GiB conversion using awk
    size_human=$(awk "BEGIN {printf \"%.1fGiB\", $total_size/1024/1024/1024}")
  fi
  
  # Format downloads with thousands separator
  local downloads_human
  if command -v numfmt >/dev/null 2>&1; then
    downloads_human=$(echo "$downloads" | numfmt --grouping 2>/dev/null || echo "$downloads")
  else
    downloads_human="$downloads"
  fi
  
  # Output: id|downloads|size|quant_count
  echo "${id}|${downloads_human}|${size_human}|${quant_count}"
}

# ---------------------------------------------------------------------------
# list_gguf_files -- list all GGUF files for a specific model
# Args: $1 = model JSON object
# Returns: JSON array of GGUF files with name, size, quant type
# Note: Extracts quant from filename using comprehensive regex patterns
# ---------------------------------------------------------------------------
list_gguf_files() {
  local model_json="$1"
  
  # Extract GGUF files with metadata
  # Quant detection regex (from filename):
  #   - Full precision: bf16, f16, f32
  #   - K-quants: Q8_0, Q6_K, Q5_K_M, Q4_K_S, etc.
  #   - Legacy: Q5_1, Q4_0, etc.
  #   - IQuant: IQ4_XS, IQ3_XXS, etc.
  echo "$model_json" | jq -r '
    .siblings // [] | 
    map(select(.rfilename | endswith(".gguf"))) |
    map({
      filename: .rfilename,
      size: .size,
      quant: (
        .rfilename | ascii_downcase |
        if test("bf16") then "BF16"
        elif test("f32") then "F32"
        elif test("f16") then "F16"
        elif test("q8_0") then "Q8_0"
        elif test("q6_k_l") then "Q6_K_L"
        elif test("q6_k") then "Q6_K"
        elif test("q5_k_m") then "Q5_K_M"
        elif test("q5_k_s") then "Q5_K_S"
        elif test("q5_1") then "Q5_1"
        elif test("q5_0") then "Q5_0"
        elif test("q4_k_m") then "Q4_K_M"
        elif test("q4_k_s") then "Q4_K_S"
        elif test("q4_1") then "Q4_1"
        elif test("q4_0") then "Q4_0"
        elif test("q3_k_l") then "Q3_K_L"
        elif test("q3_k_m") then "Q3_K_M"
        elif test("q3_k_s") then "Q3_K_S"
        elif test("q2_k") then "Q2_K"
        elif test("iq4_xs") then "IQ4_XS"
        elif test("iq4_nl") then "IQ4_NL"
        elif test("iq3_xxs") then "IQ3_XXS"
        elif test("iq3_xs") then "IQ3_XS"
        elif test("iq2_xxs") then "IQ2_XXS"
        elif test("iq2_xs") then "IQ2_XS"
        elif test("iq1_s") then "IQ1_S"
        elif test("mxfp4") then "MXFP4"
        else "UNKNOWN"
        end
      )
    })
  '
}

# ===========================================================================
# END PHASE 2
# ===========================================================================

# ===========================================================================
# PHASE 3: Minimal Mode Implementation
# ===========================================================================

# ---------------------------------------------------------------------------
# check_existing_file -- check if file already exists and prompt to overwrite
# Args: $1 = dest_dir (model directory)
#       $2 = filename (GGUF filename)
# Returns: 0 if download should proceed, 1 if should skip
# Sets: FORCE_DOWNLOAD=true to skip prompt (for --force flag)
# ---------------------------------------------------------------------------
check_existing_file() {
  local dest_dir="$1"
  local filename="$2"
  local file_path="${dest_dir}/${filename}"
  
  # If file doesn't exist, proceed with download
  if [[ ! -f "$file_path" ]]; then
    return 0
  fi
  
  # If FORCE_DOWNLOAD is set, skip prompt and proceed
  if [[ "${FORCE_DOWNLOAD:-false}" == "true" ]]; then
    warn "File exists, but --force flag set. Re-downloading..."
    return 0
  fi
  
  # File exists, get its info
  local file_size
  file_size=$(stat -f%z "$file_path" 2>/dev/null || stat -c%s "$file_path" 2>/dev/null || echo "0")
  local size_human
  size_human=$(format_size "$file_size")
  
  echo
  warn "File already exists: ${BOLD}${filename}${RESET}"
  echo "  Location: ${file_path}"
  echo "  Size: ${size_human}"
  echo
  
  # Prompt user
  read -p "Re-download this file? [y/N] " -n 1 -r
  echo
  
  if [[ $REPLY =~ ^[Yy]$ ]]; then
    info "Re-downloading..."
    return 0
  else
    info "Skipping download. File already exists."
    return 1
  fi
}

# ---------------------------------------------------------------------------
# download_gguf_file -- download a GGUF file from HuggingFace with progress
# Args: $1 = model_id (e.g. "bartowski/Llama-3.1-8B-GGUF")
#       $2 = filename (e.g. "Llama-3.1-8B-Q4_K_M.gguf")
#       $3 = dest_dir (directory to save to)
# Returns: 0 on success, 1 on failure
# Note: Uses curl with progress bar, similar to pull.sh download_file()
# ---------------------------------------------------------------------------
show_download_progress() {
  local pid="$1"
  local dest_path="$2"
  local total_size="$3"
  local bar_width=30
  local last_percent=-1
  local spin_idx=0
  local spin_chars=('-' '\' '|' '/')

  while kill -0 "$pid" 2>/dev/null; do
    local current_size=0
    if [[ -f "$dest_path" ]]; then
      current_size=$(stat -c%s "$dest_path" 2>/dev/null \
                  || stat -f%z "$dest_path" 2>/dev/null \
                  || echo 0)
    fi

    # human-readable downloaded size
    local dl_str
    if   [[ "$current_size" -ge 1073741824 ]]; then dl_str="$(awk "BEGIN {printf \"%.1fG\", $current_size/1073741824}")"
    elif [[ "$current_size" -ge 1048576    ]]; then dl_str="$(awk "BEGIN {printf \"%.0fM\", $current_size/1048576}")"
    else                                               dl_str="$(awk "BEGIN {printf \"%.0fK\", $current_size/1024}")"
    fi

    if [[ "$total_size" =~ ^[0-9]+$ ]] && [[ "$total_size" -gt 0 ]]; then
      local percent=$(( current_size * 100 / total_size ))
      [[ "$percent" -gt 100 ]] && percent=100

      if [[ "$percent" -ne "$last_percent" ]]; then
        last_percent="$percent"
        local filled=$(( percent * bar_width / 100 ))
        local empty=$(( bar_width - filled ))
        local bar="" i
        for (( i = 0; i < filled; i++ )); do bar+="█"; done
        for (( i = 0; i < empty;  i++ )); do bar+="░"; done
        local tot_str
        if   [[ "$total_size" -ge 1073741824 ]]; then tot_str="$(awk "BEGIN {printf \"%.1fG\", $total_size/1073741824}")"
        elif [[ "$total_size" -ge 1048576    ]]; then tot_str="$(awk "BEGIN {printf \"%.0fM\", $total_size/1048576}")"
        else                                            tot_str="$(awk "BEGIN {printf \"%.0fK\", $total_size/1024}")"
        fi
        printf "\r${GREEN}%3d%%${RESET}|${CYAN}%s${RESET}| ${BOLD}%s${RESET}/${BOLD}%s${RESET}  " \
          "$percent" "$bar" "$dl_str" "$tot_str"
      fi
    else
      local spin="${spin_chars[$((spin_idx % 4))]}"
      (( spin_idx++ )) || true
      printf "\r${CYAN}%s${RESET} Downloading... ${BOLD}%s${RESET}       " "$spin" "$dl_str"
    fi

    sleep 0.2
  done
}

download_gguf_file() {
  local model_id="$1"
  local filename="$2"
  local dest_dir="$3"
  
  # HuggingFace direct download URL pattern
  local url="https://huggingface.co/${model_id}/resolve/main/${filename}"
  local dest_path="${dest_dir}/${filename}"
  
  # Create destination directory
  mkdir -p "$dest_dir"
  
  # Check if file already exists
  if ! check_existing_file "$dest_dir" "$filename"; then
    return 0  # User chose not to re-download
  fi
  
  echo
  info "Downloading: ${BOLD}${filename}${RESET}"
  info "From: ${model_id}"
  echo
  
  # Download with curl and custom progress bar
  # Strategy: Start curl in background, monitor file size in real-time

  # Fetch file size before starting download.
  # HuggingFace CDN strips Content-Length from HEAD on large files, but always
  # sets Content-Range when a byte-range is requested.
  # Format: "Content-Range: bytes 0-0/<total>" → $1=header $2=bytes $3=0-0/<total>
  local total_size=0
  local range_header
  range_header=$(curl -sI -L --max-redirs 10 -H "Range: bytes=0-0" "$url" 2>/dev/null | tr -d '\r')
  total_size=$(echo "$range_header" \
    | awk 'tolower($1) == "content-range:" { split($3, a, "/"); print a[2]+0 }')
  # Fallback: Content-Length (works when no redirect strips it)
  if [[ -z "$total_size" ]] || [[ "$total_size" -eq 0 ]]; then
    total_size=$(echo "$range_header" \
      | awk 'tolower($1) == "content-length:" { last = $2+0 } END { print (last ? last : 0) }')
  fi
  : "${total_size:=0}"

  # Start download in background, silently
  curl -L -f -s -o "$dest_path" "$url" 2>/dev/null &
  local curl_pid=$!

  show_download_progress "$curl_pid" "$dest_path" "$total_size"
  
  # Wait for curl to finish and get exit code
  wait "$curl_pid"
  
  local exit_code=$?
  
  if [[ $exit_code -eq 0 ]] && [[ -f "$dest_path" ]] && [[ -s "$dest_path" ]]; then
    # Show 100% completion bar
    local final_size bar i
    final_size=$(stat -c%s "$dest_path" 2>/dev/null || stat -f%z "$dest_path" 2>/dev/null || echo 0)
    bar=""
    for (( i = 0; i < 30; i++ )); do bar+="█"; done
    local final_str
    if   [[ "$final_size" -ge 1073741824 ]]; then final_str="$(awk "BEGIN {printf \"%.1fG\", $final_size/1073741824}")"
    elif [[ "$final_size" -ge 1048576    ]]; then final_str="$(awk "BEGIN {printf \"%.0fM\", $final_size/1048576}")"
    else                                           final_str="$(awk "BEGIN {printf \"%.0fK\", $final_size/1024}")"
    fi
    printf "\r${GREEN}%3d%%${RESET}|${CYAN}%s${RESET}| ${BOLD}%s${RESET}/${BOLD}%s${RESET}  \n" \
      "100" "$bar" "$final_str" "$final_str"

    echo
    success "Downloaded to: ${dest_path}"
    
    # Save metadata
    save_download_metadata "$dest_dir" "$model_id" "$filename" "$url"
    
    return 0
  else
    echo
    rm -f "$dest_path"
    error "Download failed from: ${url}"
  fi
}

# ---------------------------------------------------------------------------
# download_aria2c_file -- download using aria2c for faster multi-connection
# Args: $1 = model_id (e.g. "bartowski/Llama-3.1-8B-GGUF")
#       $2 = filename (e.g. "Llama-3.1-8B-Q4_K_M.gguf")
#       $3 = dest_dir (directory to save to)
# Returns: 0 on success, 1 on failure
# Note: Uses aria2c with 16 connections for faster downloads
# ---------------------------------------------------------------------------
download_aria2c_file() {
  local model_id="$1"
  local filename="$2"
  local dest_dir="$3"
  
  # HuggingFace direct download URL pattern
  local url="https://huggingface.co/${model_id}/resolve/main/${filename}"
  local dest_path="${dest_dir}/${filename}"
  
  # Create destination directory
  mkdir -p "$dest_dir"
  
  # Check if file already exists
  if ! check_existing_file "$dest_dir" "$filename"; then
    return 0  # User chose not to re-download
  fi
  
  echo
  info "Downloading: ${BOLD}${filename}${RESET}"
  info "From: ${model_id}"
  info "Using aria2c (16 connections)"
  echo

  # Fetch file size BEFORE starting aria2c.
  # HuggingFace CDN (Cloudflare) strips Content-Length from HEAD responses on
  # large files, but always sets Content-Range when a byte-range is requested.
  # Format: "Content-Range: bytes 0-0/<total>" → $1=header $2=bytes $3=0-0/<total>
  local total_size=0
  local range_header
  range_header=$(curl -sI -L --max-redirs 10 -H "Range: bytes=0-0" "$url" 2>/dev/null | tr -d '\r')
  total_size=$(echo "$range_header" \
    | awk 'tolower($1) == "content-range:" { split($3, a, "/"); print a[2]+0 }')
  # Fallback: Content-Length (works when no redirect strips it)
  if [[ -z "$total_size" ]] || [[ "$total_size" -eq 0 ]]; then
    total_size=$(echo "$range_header" \
      | awk 'tolower($1) == "content-length:" { last = $2+0 } END { print (last ? last : 0) }')
  fi
  : "${total_size:=0}"

  # Download with aria2c (quiet output; custom progress bar below)
  # Options:
  #   -x16: Use 16 connections per server
  #   -s16: Split into 16 segments
  #   -j1: Download 1 file at a time
  #   --file-allocation=none: Don't pre-allocate disk space
  #   --console-log-level=error: Suppress progress spam
  #   --summary-interval=0: Disable periodic summaries
  #   --show-console-readout=false: Disable console progress output
  #   --quiet=true: Silence aria2c output (we render our own progress bar)
  #   -d: Download directory
  #   -o: Output filename
  aria2c \
    -x16 \
    -s16 \
    -j1 \
    --file-allocation=none \
    --console-log-level=error \
    --summary-interval=0 \
    --show-console-readout=false \
    --quiet=true \
    -d "$dest_dir" \
    -o "$filename" \
    "$url" &
  local aria_pid=$!

  show_download_progress "$aria_pid" "$dest_path" "$total_size"

  wait "$aria_pid"
  local exit_code=$?

  if [[ $exit_code -eq 0 ]] && [[ -f "$dest_path" ]] && [[ -s "$dest_path" ]]; then
    echo
    success "Downloaded to: ${dest_path}"
    
    # Save metadata
    save_download_metadata "$dest_dir" "$model_id" "$filename" "$url"
    
    return 0
  else
    echo
    rm -f "$dest_path"
    error "Download failed from: ${url}"
  fi
}

# ---------------------------------------------------------------------------
# save_download_metadata -- save metadata JSON for a downloaded model
# Args: $1 = dest_dir (model directory)
#       $2 = model_id (HuggingFace model ID)
#       $3 = filename (GGUF filename)
#       $4 = source_url (download URL)
# Creates: {dest_dir}/metadata.json
# ---------------------------------------------------------------------------
save_download_metadata() {
  local dest_dir="$1"
  local model_id="$2"
  local filename="$3"
  local source_url="$4"
  
  local metadata_file="${dest_dir}/metadata.json"
  local file_path="${dest_dir}/${filename}"
  
  # Get file size
  local file_size=0
  local file_size_human="N/A"
  if [[ -f "$file_path" ]]; then
    file_size=$(stat -f%z "$file_path" 2>/dev/null || stat -c%s "$file_path" 2>/dev/null || echo "0")
    file_size_human=$(format_size "$file_size")
  fi
  
  # Extract quantization from filename (case-insensitive)
  local quant="unknown"
  local filename_upper="${filename^^}"  # Convert to uppercase for matching
  if [[ "$filename_upper" =~ (Q[0-9]_[0-9K_ML]+|F16|BF16|F32|IQ[0-9]_[A-Z]+|MXFP[0-9]) ]]; then
    quant="${BASH_REMATCH[1]}"
  fi
  
  # Get quality label for quantization
  local quant_label="Other"
  if [[ "$quant" != "unknown" ]]; then
    local label_result
    label_result=$(get_quant_label "$quant")
    quant_label="${label_result%%|*}"
  fi
  
  # Get current timestamp (ISO 8601 format)
  local download_date
  download_date=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
  
  # Build metadata JSON
  # If metadata file exists, append to downloads array; otherwise create new
  if [[ -f "$metadata_file" ]]; then
    # Append to existing downloads (avoid duplicates)
    local temp_file="${metadata_file}.tmp"
    jq --arg date "$download_date" \
       --arg url "$source_url" \
       --arg file "$filename" \
       --argjson size "$file_size" \
       --arg size_human "$file_size_human" \
       --arg quant "$quant" \
       --arg label "$quant_label" \
       '.downloads |= (map(select(.filename != $file)) + [{
         downloaded_at: $date,
         filename: $file,
         source_url: $url,
         file_size: $size,
         file_size_human: $size_human,
         quantization: $quant,
         quality_label: $label
       }])' "$metadata_file" > "$temp_file"
    mv "$temp_file" "$metadata_file"
  else
    # Create new metadata file
    cat > "$metadata_file" <<EOF
{
  "model_id": "${model_id}",
  "storage_path": "${dest_dir}",
  "downloads": [
    {
      "downloaded_at": "${download_date}",
      "filename": "${filename}",
      "source_url": "${source_url}",
      "file_size": ${file_size},
      "file_size_human": "${file_size_human}",
      "quantization": "${quant}",
      "quality_label": "${quant_label}"
    }
  ]
}
EOF
  fi
  
  info "Metadata saved: ${metadata_file}"
}

# ---------------------------------------------------------------------------
# format_size -- convert bytes to human-readable size
# Args: $1 = size in bytes
# Returns: formatted string like "4.2GiB" or "512MiB"
# ---------------------------------------------------------------------------
format_size() {
  local bytes="$1"
  
  # Handle null, empty, or non-numeric values
  if [[ -z "$bytes" ]] || [[ "$bytes" == "null" ]] || ! [[ "$bytes" =~ ^[0-9]+$ ]]; then
    echo "N/A"
    return 0
  fi
  
  if [[ "$bytes" -eq 0 ]]; then
    echo "N/A"
  elif command -v numfmt >/dev/null 2>&1; then
    numfmt --to=iec-i --suffix=B "$bytes" 2>/dev/null || echo "${bytes}B"
  else
    # Fallback: simple conversion using awk
    if [[ "$bytes" -ge 1073741824 ]]; then
      awk "BEGIN {printf \"%.1fGiB\", $bytes/1073741824}"
    elif [[ "$bytes" -ge 1048576 ]]; then
      awk "BEGIN {printf \"%.0fMiB\", $bytes/1048576}"
    else
      awk "BEGIN {printf \"%.0fKiB\", $bytes/1024}"
    fi
  fi
}

# ---------------------------------------------------------------------------
# get_quant_label -- classify quantization quality
# Args: $1 = quantization string (e.g. "Q4_K_M", "F16")
# Returns: label|color (e.g. "Lossless|GREEN" or "Lossy|RED")
# Note: Smaller models suffer more from quantization. On models < 7B, prefer Q6_K minimum.
# ---------------------------------------------------------------------------
get_quant_label() {
  local quant="$1"
  
  # Lossless / Full Precision
  # No loss, identical to original weights. Too large for most local machines.
  case "$quant" in
    F32|F16|BF16)
      echo "Lossless|${GREEN}"
      return 0
      ;;
  esac
  
  # Near-Lossless (imperceptible difference)
  # Quality almost identical to F16, loss < 0.1%. Sweet spot if you have the VRAM.
  case "$quant" in
    Q8_0|Q6_K|Q6_K_L)
      echo "Near-Lossless|${CYAN}"
      return 0
      ;;
  esac
  
  # Recommended / Balanced (best quality/size tradeoff)
  # Where most people land. Q4_K_M is the llama.cpp default recommendation.
  # Q5_1 is legacy llama.cpp format, still solid.
  case "$quant" in
    Q5_K_M|Q5_K_S|Q5_1|Q4_K_M)
      echo "Recommended|${YELLOW}"
      return 0
      ;;
  esac
  
  # Fast / Compressed (noticeable but acceptable loss)
  # Starts to be perceptible on precise tasks, but usable for daily work.
  # IQ4 formats use importance matrix for better quality than K-quant equivalents.
  # Q4_1 is legacy llama.cpp format.
  case "$quant" in
    Q4_K_S|Q4_1|Q4_0|IQ4_XS|IQ4_NL)
      echo "Fast|${YELLOW}"
      return 0
      ;;
  esac
  
  # Lossy / Aggressive (significant quality loss)
  # Reserved for very limited machines. Quality takes a serious hit, especially on small models (< 7B).
  # Q3_K_M and Q3_K_S have minimal difference, both degrade noticeably.
  case "$quant" in
    Q3_K_L|Q3_K_M|Q3_K_S|Q2_K|Q2_K_L|IQ3_XXS|IQ3_XS|IQ2_XXS|IQ2_XS)
      echo "Lossy|${RED}"
      return 0
      ;;
  esac
  
  # Experimental
  # Very recent or extreme formats. MXFP4 is Microsoft's format for specific NPU/GPU.
  # IQ1_S is 1-bit quantization, mainly for research.
  case "$quant" in
    MXFP4|IQ1_S)
      echo "Experimental|${MAGENTA}"
      return 0
      ;;
  esac
  
  # Unknown/Other
  echo "Other|${DIM}"
}

# ---------------------------------------------------------------------------
# quant_rank -- sort order for quantizations (lower is better)
# Args: $1 = quantization string (e.g. "Q4_K_M")
# Returns: numeric rank
# ---------------------------------------------------------------------------
quant_rank() {
  local quant="$1"

  case "$quant" in
    F32) echo 0 ;;
    BF16) echo 1 ;;
    F16) echo 2 ;;
    Q8_0) echo 10 ;;
    Q6_K_L) echo 20 ;;
    Q6_K) echo 21 ;;
    Q5_K_M) echo 30 ;;
    Q5_K_S) echo 31 ;;
    Q5_1) echo 32 ;;
    Q4_K_M) echo 33 ;;
    Q4_K_S) echo 40 ;;
    Q4_1) echo 41 ;;
    Q4_0) echo 42 ;;
    IQ4_NL) echo 43 ;;
    IQ4_XS) echo 44 ;;
    Q3_K_L) echo 50 ;;
    Q3_K_M) echo 51 ;;
    Q3_K_S) echo 52 ;;
    Q2_K_L) echo 53 ;;
    Q2_K) echo 54 ;;
    IQ3_XXS) echo 55 ;;
    IQ3_XS) echo 56 ;;
    IQ2_XXS) echo 57 ;;
    IQ2_XS) echo 58 ;;
    IQ1_S) echo 90 ;;
    MXFP4) echo 91 ;;
    *) echo 999 ;;
  esac
}

# ===========================================================================
# END PHASE 3
# ===========================================================================

# ---------------------------------------------------------------------------
# main -- entry point
# ---------------------------------------------------------------------------
main() {
  # Parse arguments
  local query=""
  local action="interactive"
  
  while [[ $# -gt 0 ]]; do
    case "$1" in
      -h|--help)
        usage
        ;;
      --version)
        echo "llama-models v${VERSION}"
        exit 0
        ;;
      --mode)
        MODE="$2"
        shift 2
        ;;
      --install-deps)
        install_dependencies
        exit 0
        ;;
      --force)
        FORCE_DOWNLOAD=true
        shift
        ;;
      search)
        action="search"
        query="$2"
        shift 2
        ;;
      list)
        action="list"
        shift
        ;;
      sync)
        action="sync"
        shift
        ;;
      *)
        query="$1"
        action="search"
        shift
        ;;
    esac
  done
  
  # Detect mode if not forced
  if [[ -z "$MODE" ]]; then
    detect_mode "$action"
  fi
  
  # Only show mode info for interactive/search commands (skip for list and sync)
  if [[ "$action" != "list" ]] && [[ "$action" != "sync" ]]; then
    info "Running in ${BOLD}${MODE}${RESET} mode"
  fi
  
  # Create models directory
  mkdir -p "$MODELS_DIR"
  
  # Execute action based on mode
  case "$action" in
    interactive)
      if [[ "$MODE" == "premium" ]]; then
        run_premium_interactive
      else
        run_minimal_interactive
      fi
      ;;
    search)
      if [[ "$MODE" == "premium" ]]; then
        run_premium_search "$query"
      else
        run_minimal_search "$query"
      fi
      ;;
    list)
      list_local_models
      ;;
    sync)
      sync_local_models
      ;;
  esac
}

# ---------------------------------------------------------------------------
# run_premium_interactive -- home screen with gum-based navigation menu
# ---------------------------------------------------------------------------
run_premium_interactive() {
  while true; do
    clear
    gum style \
      --border double \
      --border-foreground 212 \
      --padding "1 4" \
      --margin "0 2" \
      "llama-models v${VERSION}" \
      "HuggingFace GGUF Browser"
    echo

    local choice
    choice=$(gum choose \
      --header "What would you like to do?" \
      --header.foreground 212 \
      --cursor.foreground 212 \
      "Search models" \
      "Browse popular models" \
      "List local models" \
      "Delete a model" \
      "Quit") || choice="Quit"

    case "$choice" in
      "Search models")
        echo
        local query
        query=$(gum input \
          --placeholder "Type a model name or keyword..." \
          --prompt "Search > " \
          --prompt.foreground 212 \
          --width 60) || query=""
        echo
        run_premium_search "$query"
        ;;
      "Browse popular models")
        echo
        run_premium_search ""
        ;;
      "List local models")
        echo
        list_local_models
        ;;
      "Delete a model")
        echo
        delete_local_model
        ;;
      *)
        exit 0
        ;;
    esac
  done
}

# ---------------------------------------------------------------------------
# run_premium_search -- search and browse models with gum filter + pagination
# Args: $1 = search query (empty = popular models sorted by downloads)
# Loop: re-shows filter after returning from quantization view (no re-fetch).
#       "Load more" fetches the next page and appends to the list.
# ---------------------------------------------------------------------------
run_premium_search() {
  local query="${1:-}"
  local limit=50
  local offset=0
  local -a all_model_ids=()
  local -a all_model_jsons=()
  local -a all_display_lines=()
  local has_more=false
  local need_fetch=true

  while true; do
    # -- Fetch a page of results ---------------------------------------------
    if [[ "$need_fetch" == "true" ]]; then
      need_fetch=false

      # Build the HuggingFace API URL
      local params="limit=${limit}&sort=downloads&direction=-1&full=true&filter=gguf"
      if [[ -n "$query" ]]; then
        local encoded_query
        encoded_query=$(printf '%s' "$query" | sed 's/ /%20/g')
        params="${params}&search=${encoded_query}"
      fi
      if [[ "$offset" -gt 0 ]]; then
        params="${params}&skip=${offset}"
      fi
      local full_url="${HF_API_URL}?${params}"

      # Fetch with a gum spinner so the user sees progress
      local page_results=""
      if ! page_results=$(gum spin \
          --spinner dot \
          --title "Fetching GGUF models from HuggingFace..." \
          --show-output \
          -- curl -fsSL --max-time 30 "$full_url" 2>/dev/null); then
        gum style --foreground 196 "?-- Failed to query HuggingFace API. Check your connection."
        return 1
      fi

      if ! echo "$page_results" | jq empty 2>/dev/null; then
        gum style --foreground 196 "?-- Invalid API response from HuggingFace."
        return 1
      fi

      local page_count
      page_count=$(echo "$page_results" | jq 'length')

      if [[ "$page_count" -eq 0 ]]; then
        if [[ $offset -eq 0 ]]; then
          echo
          gum style --foreground 208 "No GGUF models found${query:+ matching: \"${query}\"}"
          gum style --foreground 245 "Try a different search term."
          return 0
        fi
        # No more pages -- display accumulated results without "load more"
        has_more=false
      else
        # Accumulate results from this page into the master arrays
        local -a page_jsons=()
        mapfile -t page_jsons < <(echo "$page_results" | jq -c '.[]')

        has_more=false
        if [[ ${#page_jsons[@]} -ge $limit ]]; then
          has_more=true
        fi

        for json in "${page_jsons[@]}"; do
          local metadata
          metadata=$(parse_model_metadata "$json")
          local model_id downloads size quant_count
          IFS='|' read -r model_id downloads size quant_count <<< "$metadata"

          all_model_ids+=("$model_id")
          all_model_jsons+=("$json")

          local display_line
          printf -v display_line "%-52s  %12s dl  %3s variants" \
            "$model_id" "$downloads" "$quant_count"
          all_display_lines+=("$display_line")
        done
      fi
    fi

    # -- Present gum filter --------------------------------------------------
    clear
    gum style \
      --bold \
      --foreground 212 \
      "  ${#all_model_ids[@]} GGUF models${query:+ matching \"${query}\"}"
    gum style \
      --foreground 245 \
      "  Start typing to filter  |  up/dn navigate  |  Enter select  |  Esc back"
    echo

    local -a filter_items=("${all_display_lines[@]}")
    [[ "$has_more" == "true" ]] && filter_items+=("-- Load 50 more results --")
    filter_items+=("-- Back to menu --")

    local selected=""
    selected=$(printf "%s\n" "${filter_items[@]}" | \
      gum filter \
        --placeholder "Fuzzy search models..." \
        --prompt "  >  " \
        --prompt.foreground 212 \
        --match.foreground 212 \
        --height 20 \
        --indicator=">" \
        --indicator.foreground 212) || selected=""

    case "$selected" in
      "-- Back to menu --"|"")
        return 0
        ;;
      "-- Load 50 more results --")
        offset=$((offset + limit))
        need_fetch=true
        continue
        ;;
    esac

    # The first whitespace-delimited token of each display line is the model_id
    local selected_model_id
    selected_model_id=$(printf '%s' "$selected" | awk '{print $1}')
    local selected_index=-1
    for i in "${!all_model_ids[@]}"; do
      if [[ "${all_model_ids[i]}" == "$selected_model_id" ]]; then
        selected_index=$i
        break
      fi
    done

    if [[ $selected_index -ge 0 ]] && [[ $selected_index -lt ${#all_model_ids[@]} ]]; then
      show_premium_model_quantizations \
        "${all_model_jsons[$selected_index]}" \
        "${all_model_ids[$selected_index]}"
      # After returning (download done or Back selected), the while loop
      # continues and re-presents the same filter -- no re-fetch needed.
    fi
  done
}

# ---------------------------------------------------------------------------
# show_premium_model_quantizations -- select and download a quantization
# Args: $1 = model JSON object
#       $2 = model_id
# Returns 0 when done (download complete, user chose Back, or cancelled).
# Callers should loop back to re-show search results after this returns.
# ---------------------------------------------------------------------------
show_premium_model_quantizations() {
  local model_json="$1"
  local model_id="$2"

  echo
  gum style \
    --border rounded \
    --border-foreground 86 \
    --padding "1 2" \
    --margin "1 0" \
    "Model: ${model_id}"
  echo

  # Fetch GGUF files; refresh sizes from the detail endpoint if any are missing
  local gguf_files
  gguf_files=$(list_gguf_files "$model_json")

  if echo "$gguf_files" | jq -e 'any(.[]; .size == null or .size == 0)' >/dev/null 2>&1; then
    local refreshed_json=""
    if refreshed_json=$(gum spin \
        --spinner dot \
        --title "Fetching file metadata..." \
        --show-output \
        -- curl -fsSL --max-time 30 \
          "https://huggingface.co/api/models/${model_id}?full=true" 2>/dev/null); then
      if echo "$refreshed_json" | jq empty 2>/dev/null; then
        gguf_files=$(list_gguf_files "$refreshed_json")
      fi
    fi
  fi

  if [[ -z "$gguf_files" ]] || [[ "$gguf_files" == "[]" ]]; then
    gum style --foreground 208 "No GGUF files found for this model."
    return 1
  fi

  # Parse GGUF file list into parallel arrays
  local -a filenames=() sizes=() quants=()
  local -a gguf_array=()
  mapfile -t gguf_array < <(echo "$gguf_files" | jq -c '.[]')

  for item in "${gguf_array[@]}"; do
    filenames+=("$(echo "$item" | jq -r '.filename')")
    sizes+=("$(echo "$item" | jq -r '.size // 0')")
    quants+=("$(echo "$item" | jq -r '.quant')")
  done

  # Sort by quality tier ascending, then by file size descending (best first)
  if [[ ${#quants[@]} -gt 1 ]]; then
    local -a sort_keys=()
    for i in "${!quants[@]}"; do
      local sz="${sizes[i]}"
      [[ "$sz" =~ ^[0-9]+$ ]] || sz=0
      sort_keys+=("$(quant_rank "${quants[i]}")|${sz}|${i}")
    done
    local -a sfn=() ssz=() sqt=()
    IFS=$'\n' read -r -d '' -a sort_keys < <(
      printf '%s\n' "${sort_keys[@]}" | sort -t'|' -k1,1n -k2,2nr
      printf '\0'
    )
    for key in "${sort_keys[@]}"; do
      local idx="${key##*|}"
      sfn+=("${filenames[$idx]}")
      ssz+=("${sizes[$idx]}")
      sqt+=("${quants[$idx]}")
    done
    filenames=("${sfn[@]}")
    sizes=("${ssz[@]}")
    quants=("${sqt[@]}")
  fi

  # Display a rich colour-coded quantization table once.
  echo -e "${BOLD}  Quant        Quality         Filename${RESET}"
  echo   "  ----------------------------------------------------------------"
  for i in "${!quants[@]}"; do
    local label_text label_color label_info
    label_info=$(get_quant_label "${quants[i]}")
    IFS='|' read -r label_text label_color <<< "$label_info"
    printf "  %-12s ${label_color}%-16s${RESET} %s\n" \
      "${quants[i]}" "$label_text" "${filenames[i]}"
  done
  echo

  # gum choose with plain quant + filename (no ANSI — gum strips them anyway).
  # The colored table above already gave the user quality info; the menu just
  # lets them pick directly without a numeric indirection.
  local -a choose_items=()
  for i in "${!quants[@]}"; do
    local item
    printf -v item "%-12s  %s" "${quants[i]}" "${filenames[i]}"
    choose_items+=("$item")
  done
  choose_items+=("-- Back to results --" "-- Quit --")

  local choice=""
  choice=$(printf "%s\n" "${choose_items[@]}" | \
    gum choose \
      --header "Select a quantization to download:" \
      --header.foreground 212 \
      --cursor.foreground 212) || choice=""

  case "$choice" in
    "-- Back to results --"|"")
      return 0
      ;;
    "-- Quit --")
      exit 0
      ;;
  esac

  # Map choice back to its array index by matching the generated item string.
  local selected_index=-1
  for i in "${!choose_items[@]}"; do
    if [[ "${choose_items[i]}" == "$choice" ]]; then
      selected_index=$i
      break
    fi
  done

  # Guard: ensure it's a valid quant index (not a sentinel)
  if [[ $selected_index -lt 0 ]] || [[ $selected_index -ge ${#quants[@]} ]]; then
    return 0
  fi

  local filename="${filenames[$selected_index]}"
  local quant="${quants[$selected_index]}"

  # Confirm before downloading
  echo
  gum style \
    --border rounded \
    --border-foreground 212 \
    --padding "0 2" \
    "  File  : ${filename}" \
    "  Quant : ${quant}"
  echo

  if ! gum confirm \
    --affirmative "  Download  " \
    --negative "  Cancel  " \
    --default=true \
    "Download this file?"; then
    info "Download cancelled."
    return 0
  fi

  # Perform the download
  local sanitized_id
  sanitized_id=$(echo "$model_id" | tr '/' '_')
  local dest_dir="${MODELS_DIR}/${sanitized_id}"

  echo
  if check_dependency aria2c; then
    download_aria2c_file "$model_id" "$filename" "$dest_dir"
  else
    download_gguf_file "$model_id" "$filename" "$dest_dir"
  fi

  # Success screen
  echo
  gum style \
    --border double \
    --border-foreground 82 \
    --padding "1 2" \
    --margin "1 0" \
    "[OK]  Download Complete" \
    "" \
    "  Model : ${model_id}" \
    "  File  : ${filename}" \
    "  Path  : ${dest_dir}"
  echo
  gum style --foreground 245 "Run with:"
  echo
  echo "  llama-cli -m ${dest_dir}/${filename} -cnv -t 8 -c 8192 --temp 0.7"
  echo

  # Ask whether to keep browsing or exit
  if gum confirm \
    --affirmative "  Browse more  " \
    --negative "  Exit  " \
    --default=false \
    "Download another model?"; then
    return 0
  else
    exit 0
  fi
}

# ---------------------------------------------------------------------------
# run_minimal_interactive -- interactive mode with basic prompts
# ---------------------------------------------------------------------------
run_minimal_interactive() {
  echo
  echo -e "${BOLD}llama-models${RESET} -- Interactive Model Browser"
  echo
  
  # Prompt for search query
  read -p "Enter search query (or press Enter for popular models): " query
  
  run_minimal_search "$query"
}

# ---------------------------------------------------------------------------
# run_minimal_search -- search and select models with bash select menu
# Args: $1 = search query
# ---------------------------------------------------------------------------
run_minimal_search() {
  local query="$1"
  local limit=20
  local offset=0
  local has_more=true
  
  # Initialize arrays to accumulate results across pages
  local -a all_model_ids=()
  local -a all_model_metadata=()
  local -a all_model_jsons=()
  
  # Main pagination loop
  while true; do
    # Search models via HuggingFace API with offset for pagination
    local results
    results=$(search_models "$query" "$limit" "$offset")
    
    if [[ -z "$results" ]] || [[ "$results" == "[]" ]]; then
      if [[ $offset -eq 0 ]]; then
        # No results on first page
        echo
        warn "No GGUF models found for query: ${query}"
        echo
        echo "Try a different search term, or browse popular models with:"
        echo "  llama-models search"
        exit 0
      else
        # No more results on subsequent pages
        has_more=false
        break
      fi
    fi
    
    # Parse results into arrays
    local -a model_jsons
    
    echo
    info "Parsing results..."
    
    # Use mapfile to avoid process substitution issues
    mapfile -t model_jsons < <(echo "$results" | jq -c '.[]')
    
    # Check if we got fewer results than requested (means this is the last page)
    if [[ ${#model_jsons[@]} -lt $limit ]]; then
      has_more=false
    fi
    
    # Process each model and append to accumulated arrays
    for i in "${!model_jsons[@]}"; do
      local metadata
      metadata=$(parse_model_metadata "${model_jsons[i]}")
      
      # Extract just the model ID for display
      local model_id
      model_id=$(echo "$metadata" | cut -d'|' -f1)
      
      all_model_ids+=("$model_id")
      all_model_metadata+=("$metadata")
      all_model_jsons+=("${model_jsons[i]}")
    done
    
    # Display all accumulated results in a formatted table
    echo
    echo -e "${BOLD}Found ${#all_model_ids[@]} GGUF models:${RESET}"
    echo
    echo -e "${BOLD}#   Model ID                                Downloads   Variants${RESET}"
    echo "-----------------------------------------------------------------------------"
    
    for i in "${!all_model_ids[@]}"; do
      local metadata="${all_model_metadata[i]}"
      local model_id downloads size quant_count
      
      IFS='|' read -r model_id downloads size quant_count <<< "$metadata"
      
      # Truncate model_id if too long
      local display_id="$model_id"
      if [[ ${#model_id} -gt 38 ]]; then
        display_id="${model_id:0:35}..."
      fi
      
      printf "${CYAN}%-3d${RESET} %-38s %10s   ${BOLD}%8s${RESET}\n" \
        "$((i+1))" "$display_id" "$downloads" "$quant_count"
    done
    
    echo
    
    # Build select menu options
    local -a select_options=("${all_model_ids[@]}")
    
    # Add pagination/quit options
    if [[ "$has_more" == "true" ]]; then
      select_options+=("-- Show more --")
    fi
    select_options+=("Quit")
    
    # Bash select menu for model selection
    echo -e "${BOLD}Select a model to download:${RESET}"
    
    PS3="Enter number: "
    local choice
    select choice in "${select_options[@]}"; do
      if [[ -n "$choice" ]]; then
        if [[ "$choice" == "Quit" ]]; then
          echo
          info "Exiting."
          exit 0
        elif [[ "$choice" == "-- Show more --" ]]; then
          # Load next page
          offset=$((offset + limit))
          echo
          info "Loading more results..."
          break
        else
          # User selected a model
          # Find the selected model's index
          local selected_index=-1
          for i in "${!all_model_ids[@]}"; do
            if [[ "${all_model_ids[i]}" == "$choice" ]]; then
              selected_index=$i
              break
            fi
          done
          
          if [[ $selected_index -ge 0 ]]; then
            show_model_quantizations "${all_model_jsons[selected_index]}" "$choice"
            return 0
          fi
        fi
      else
        echo "Invalid selection. Try again or press Ctrl+C to exit."
      fi
    done
    
    # If we broke out of select to load more, continue the while loop
    if [[ "$choice" == "-- Show more --" ]]; then
      continue
    else
      # If we selected a model, we returned above, so this should not happen
      break
    fi
  done
}

# ---------------------------------------------------------------------------
# show_model_quantizations -- display available quants and let user pick one
# Args: $1 = model JSON object
#       $2 = model_id (for display)
# ---------------------------------------------------------------------------
show_model_quantizations() {
  local model_json="$1"
  local model_id="$2"
  
  echo
  echo -e "${BOLD}Model:${RESET} ${model_id}"
  echo
  
  # Get list of GGUF files
  local gguf_files
  gguf_files=$(list_gguf_files "$model_json")
  
  if [[ -z "$gguf_files" ]] || [[ "$gguf_files" == "[]" ]]; then
    warn "No GGUF files found in this model"
    exit 1
  fi
  
  # Parse into arrays
  local -a filenames
  local -a sizes
  local -a quants
  
  # Use mapfile to avoid process substitution deadlock
  local -a gguf_array
  mapfile -t gguf_array < <(echo "$gguf_files" | jq -c '.[]')
  
  for i in "${!gguf_array[@]}"; do
    local filename size quant
    filename=$(echo "${gguf_array[i]}" | jq -r '.filename')
    size=$(echo "${gguf_array[i]}" | jq -r '.size')
    quant=$(echo "${gguf_array[i]}" | jq -r '.quant')
    
    filenames[i]="$filename"
    sizes[i]="$size"
    quants[i]="$quant"
  done

  # Sort quantizations by quality tier (best first), then by size (largest first)
  if [[ ${#quants[@]} -gt 1 ]]; then
    local -a sort_keys sorted_filenames sorted_sizes sorted_quants
    for i in "${!quants[@]}"; do
      local size_num="${sizes[i]}"
      if ! [[ "$size_num" =~ ^[0-9]+$ ]]; then
        size_num=0
      fi
      sort_keys+=("$(quant_rank "${quants[i]}")|${size_num}|${i}")
    done

    IFS=$'\n' read -r -d '' -a sort_keys < <(printf '%s\n' "${sort_keys[@]}" | sort -t'|' -k1,1n -k2,2nr && printf '\0')

    for key in "${sort_keys[@]}"; do
      local idx="${key##*|}"
      sorted_filenames+=("${filenames[idx]}")
      sorted_sizes+=("${sizes[idx]}")
      sorted_quants+=("${quants[idx]}")
    done

    filenames=("${sorted_filenames[@]}")
    sizes=("${sorted_sizes[@]}")
    quants=("${sorted_quants[@]}")
  fi
  
  # Display quantization options
  echo -e "${BOLD}Available quantizations:${RESET}"
  echo
  echo -e "${BOLD}#   Quant     Size        Quality     Filename${RESET}"
  echo "-----------------------------------------------------------------------------"
  
  for i in "${!filenames[@]}"; do
    local size_human
    size_human=$(format_size "${sizes[i]}")
    
    # Get quality label
    local label_info label_text label_color
    label_info=$(get_quant_label "${quants[i]}")
    IFS='|' read -r label_text label_color <<< "$label_info"
    
    # Truncate filename if too long
    local display_name="${filenames[i]}"
    if [[ ${#display_name} -gt 35 ]]; then
      display_name="${display_name:0:32}..."
    fi
    
    printf "${CYAN}%-3d${RESET} %-9s %-11s ${label_color}%-10s${RESET} %s\n" \
      "$((i+1))" "${quants[i]}" "$size_human" "$label_text" "$display_name"
  done
  
  echo
  
  # Select menu for quantization
  echo -e "${BOLD}Select a quantization to download (or 'q' to quit):${RESET}"
  
  PS3="Enter number: "
  local choice
  select choice in "${quants[@]}" "Back" "Quit"; do
    if [[ -n "$choice" ]]; then
      if [[ "$choice" == "Quit" ]]; then
        echo
        info "Exiting."
        exit 0
      elif [[ "$choice" == "Back" ]]; then
        return 0
      fi
      
      # Find selected quant's index
      local selected_index=-1
      for i in "${!quants[@]}"; do
        if [[ "${quants[i]}" == "$choice" ]]; then
          selected_index=$i
          break
        fi
      done
      
      if [[ $selected_index -ge 0 ]]; then
        # Create organized storage path: models_dir/model_id/
        local safe_model_id
        safe_model_id=$(echo "$model_id" | tr '/' '_')
        local dest_dir="${MODELS_DIR}/${safe_model_id}"
        
        # Download the selected file
        download_gguf_file "$model_id" "${filenames[selected_index]}" "$dest_dir"
        
        echo
        success "Model ready to use!"
        echo
        echo -e "Saved to: ${BOLD}${dest_dir}${RESET}"
        echo
        echo -e "${BOLD}Run with:${RESET}"
        echo
        echo "  llama-cli -m ${dest_dir}/${filenames[selected_index]} -cnv -t 8 -c 8192 --temp 0.7"
        echo
        
        exit 0
      fi
    else
      echo "Invalid selection. Try again or press Ctrl+C to exit."
    fi
  done
}

list_local_models() {
  info "Listing models in: $MODELS_DIR"
  
  if [[ ! -d "$MODELS_DIR" ]] || [[ -z "$(ls -A "$MODELS_DIR" 2>/dev/null)" ]]; then
    echo "No models downloaded yet."
    echo
    echo "Download models with: llama-models search <query>"
    echo
    echo -e "${DIM}Press any key to return to menu...${RESET}"
    read -r -s -n 1
    return 0
  fi
  
  echo
  echo -e "${BOLD}Downloaded Models${RESET}"
  echo
  echo -e "${BOLD}Model ID                                Quant      Size       Downloaded${RESET}"
  echo "-----------------------------------------------------------------------------"
  
  # Find all metadata.json files
  local found=0
  while IFS= read -r -d '' metadata_file; do
    if [[ -f "$metadata_file" ]]; then
      local model_id
      model_id=$(jq -r '.model_id // "unknown"' "$metadata_file")
      
      # Process each download in the metadata
      local downloads_count
      downloads_count=$(jq -r '.downloads | length' "$metadata_file")
      
      for ((i=0; i<downloads_count; i++)); do
        local filename quant size downloaded_at
        filename=$(jq -r ".downloads[$i].filename // \"unknown\"" "$metadata_file")
        quant=$(jq -r ".downloads[$i].quantization // \"unknown\"" "$metadata_file")
        size=$(jq -r ".downloads[$i].file_size // 0" "$metadata_file")
        downloaded_at=$(jq -r ".downloads[$i].downloaded_at // \"unknown\"" "$metadata_file")
        
        # Format size
        local size_human
        size_human=$(format_size "$size")
        
        # Format date (just date, no time)
        local date_short
        if [[ "$downloaded_at" =~ ^([0-9]{4}-[0-9]{2}-[0-9]{2}) ]]; then
          date_short="${BASH_REMATCH[1]}"
        else
          date_short="$downloaded_at"
        fi
        
        # Truncate model_id if too long
        local display_model_id="$model_id"
        if [[ ${#display_model_id} -gt 38 ]]; then
          display_model_id="${display_model_id:0:35}..."
        fi
        
        # Get quality label
        local label_info label_text label_color
        label_info=$(get_quant_label "$quant")
        IFS='|' read -r label_text label_color <<< "$label_info"
        
        printf "%-40s ${label_color}%-10s${RESET} %-10s %s\n" \
          "$display_model_id" "$quant" "$size_human" "$date_short"
        
        found=1
      done
    fi
  done < <(find "$MODELS_DIR" -type f -name "metadata.json" -print0)
  
  if [[ $found -eq 0 ]]; then
    echo "No models with metadata found."
    echo
    echo -e "${DIM}Note: Models downloaded before metadata tracking won't appear here.${RESET}"
    echo -e "${DIM}Re-download to generate metadata.${RESET}"
  fi

  echo
  echo -e "${DIM}Press any key to return to menu...${RESET}"
  read -r -s -n 1
}

# ---------------------------------------------------------------------------
# delete_local_model -- interactively select and delete a downloaded GGUF file
# Presents a gum choose list of all .gguf files, asks for confirmation,
# deletes the file, removes its entry from metadata.json, and cleans up
# empty model directories.
# ---------------------------------------------------------------------------
delete_local_model() {
  if [[ ! -d "$MODELS_DIR" ]] || [[ -z "$(ls -A "$MODELS_DIR" 2>/dev/null)" ]]; then
    echo "No models downloaded yet."
    echo
    echo -e "${DIM}Press any key to return to menu...${RESET}"
    read -r -s -n 1
    return 0
  fi

  # Collect all .gguf files with display info
  local -a file_paths=()
  local -a display_items=()

  while IFS= read -r -d '' gguf_file; do
    local item_dir item_meta item_model_id item_filename item_size item_size_human
    item_filename=$(basename "$gguf_file")
    item_dir=$(dirname "$gguf_file")
    item_meta="${item_dir}/metadata.json"

    if [[ -f "$item_meta" ]]; then
      item_model_id=$(jq -r '.model_id // ""' "$item_meta")
    else
      item_model_id=$(basename "$item_dir")
    fi

    item_size=$(stat -c%s "$gguf_file" 2>/dev/null || stat -f%z "$gguf_file" 2>/dev/null || echo 0)
    item_size_human=$(format_size "$item_size")

    file_paths+=("$gguf_file")
    display_items+=("$(printf '%-42s  %-44s  %s' "${item_model_id:0:40}" "${item_filename:0:42}" "$item_size_human")")
  done < <(find "$MODELS_DIR" -type f -name "*.gguf" -print0 2>/dev/null | sort -z)

  if [[ ${#file_paths[@]} -eq 0 ]]; then
    echo "No GGUF model files found."
    echo
    echo -e "${DIM}Press any key to return to menu...${RESET}"
    read -r -s -n 1
    return 0
  fi

  local choice
  choice=$(gum choose \
    --header "Select a model file to delete:" \
    --header.foreground 196 \
    --cursor.foreground 196 \
    "${display_items[@]}") || { echo; return 0; }

  # Match selection back to file path
  local del_path=""
  local idx
  for idx in "${!display_items[@]}"; do
    if [[ "${display_items[$idx]}" == "$choice" ]]; then
      del_path="${file_paths[$idx]}"
      break
    fi
  done

  [[ -z "$del_path" ]] && return 0

  local del_filename del_dir del_meta
  del_filename=$(basename "$del_path")
  del_dir=$(dirname "$del_path")
  del_meta="${del_dir}/metadata.json"

  # Confirmation
  echo
  warn "About to permanently delete: ${BOLD}${del_filename}${RESET}"
  echo "  Path: $del_path"
  echo

  if ! gum confirm "Delete this file?" \
      --default=false \
      --prompt.foreground=196 \
      --selected.background=196 2>/dev/null; then
    info "Cancelled."
    sleep 1
    return 0
  fi

  rm -f "$del_path"

  # Remove entry from metadata.json
  if [[ -f "$del_meta" ]]; then
    local tmp_meta="${del_meta}.tmp"
    jq --arg fn "$del_filename" \
      'del(.downloads[] | select(.filename == $fn))' \
      "$del_meta" > "$tmp_meta" && mv "$tmp_meta" "$del_meta"
  fi

  # Remove model dir if no .gguf files remain
  if [[ -z "$(find "$del_dir" -maxdepth 1 -name '*.gguf' 2>/dev/null)" ]]; then
    rm -rf "$del_dir"
  fi

  echo
  success "Deleted: ${BOLD}${del_filename}${RESET}"
  sleep 1
}

# ---------------------------------------------------------------------------
# sync_local_models -- check for updates to downloaded models
# Compares local files against HuggingFace to detect:
#   - New quantizations available
#   - Files with different sizes (possibly fixed/corrupted)
# ---------------------------------------------------------------------------
sync_local_models() {
  info "Checking for updates in: $MODELS_DIR"
  
  if [[ ! -d "$MODELS_DIR" ]] || [[ -z "$(ls -A "$MODELS_DIR" 2>/dev/null)" ]]; then
    echo "No models downloaded yet."
    echo
    echo "Download models with: llama-models search <query>"
    return 0
  fi
  
  echo
  echo -e "${BOLD}Checking for Updates${RESET}"
  echo
  
  local found_updates=false
  
  # Find all metadata.json files
  while IFS= read -r -d '' metadata_file; do
    if [[ ! -f "$metadata_file" ]]; then
      continue
    fi
    
    local model_id
    model_id=$(jq -r '.model_id // "unknown"' "$metadata_file")
    
    if [[ "$model_id" == "unknown" ]]; then
      continue
    fi
    
    echo -e "${BOLD}${model_id}${RESET}"
    
    # Fetch available files from HuggingFace
    local hf_data
    hf_data=$(fetch_model_data "$model_id" 2>/dev/null)
    
    if [[ -z "$hf_data" ]]; then
      echo -e "  ${RED}?--${RESET} Cannot fetch from HuggingFace (repo may be private or deleted)"
      echo
      continue
    fi
    
    # Extract GGUF files from HuggingFace
    local hf_files
    hf_files=$(echo "$hf_data" | jq -r '.siblings[]? | select(.rfilename | endswith(".gguf")) | .rfilename')
    
    if [[ -z "$hf_files" ]]; then
      echo -e "  ${DIM}No GGUF files found on HuggingFace${RESET}"
      echo
      continue
    fi
    
    # Get local files
    local local_downloads
    local_downloads=$(jq -r '.downloads[].filename' "$metadata_file")
    
    # Check for new quantizations
    local new_quants=()
    while IFS= read -r hf_file; do
      if ! echo "$local_downloads" | grep -q "^${hf_file}$"; then
        new_quants+=("$hf_file")
      fi
    done <<< "$hf_files"
    
    # Check for size differences (potentially fixed files)
    local updated_files=()
    while IFS= read -r local_file; do
      # Get local size
      local local_size
      local_size=$(jq -r ".downloads[] | select(.filename == \"$local_file\") | .file_size" "$metadata_file")
      
      # Get HuggingFace size
      local hf_size
      hf_size=$(echo "$hf_data" | jq -r ".siblings[]? | select(.rfilename == \"$local_file\") | .size // 0")
      
      if [[ -n "$hf_size" ]] && [[ "$hf_size" != "0" ]] && [[ "$local_size" != "$hf_size" ]]; then
        updated_files+=("$local_file (local: $(format_size "$local_size"), HF: $(format_size "$hf_size"))")
      fi
    done <<< "$local_downloads"
    
    # Report findings
    local has_changes=false
    
    if [[ ${#new_quants[@]} -gt 0 ]]; then
      echo -e "  ${GREEN}New quantizations available:${RESET}"
      for quant in "${new_quants[@]}"; do
        echo -e "    ${CYAN}+${RESET} $quant"
      done
      has_changes=true
      found_updates=true
    fi
    
    if [[ ${#updated_files[@]} -gt 0 ]]; then
      echo -e "  ${YELLOW}Files with size mismatch (possibly fixed/corrupted):${RESET}"
      for file_info in "${updated_files[@]}"; do
        echo -e "    ${YELLOW}!${RESET} $file_info"
      done
      has_changes=true
      found_updates=true
    fi
    
    if [[ "$has_changes" == "false" ]]; then
      echo -e "  ${GREEN}[OK]${RESET} Up to date"
    fi
    
    echo
  done < <(find "$MODELS_DIR" -type f -name "metadata.json" -print0)
  
  echo
  
  if [[ "$found_updates" == "true" ]]; then
    echo -e "${BOLD}To download updates:${RESET}"
    echo "  llama-models search <model-name>  # Search and select new quantizations"
    echo "  llama-models --force search <model-name>  # Re-download to fix corrupted files"
  else
    echo -e "${GREEN}All models are up to date!${RESET}"
  fi
  
  echo
}

# ---------------------------------------------------------------------------
# Execute main if script is run directly (not sourced)
# ---------------------------------------------------------------------------
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
  main "$@"
fi
